{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f684397f-1d6a-4616-8118-68d5b3586e42",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:gold\">**Árbol de Decisión con información geológica**</span>\n",
    "***\n",
    "\n",
    "### **Editado por: Kevin Alexander Gómez**\n",
    "#### Contacto: kevinalexandr19@gmail.com | [Linkedin](https://www.linkedin.com/in/kevin-alexander-g%C3%B3mez-2b0263111/) | [Github](https://github.com/kevinalexandr19)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678b92b-76e2-4abc-83ad-ef7d160faada",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Descripción**\n",
    "\n",
    "En este tutorial, revisarás un ejemplo del modelo de <span style=\"color:gold\">árboles de decisión</span> en Geología.\n",
    "\n",
    "Es necesario tener un conocimiento previo en geoquímica y petrología, también es necesario haber aprendido a usar [Pandas](1c_pandas.ipynb).\n",
    "\n",
    "Este notebook es parte del proyecto [**Python para Geólogos**](https://github.com/kevinalexandr19/manual-python-geologia), y ha sido creado con la finalidad de facilitar el aprendizaje en Python para estudiantes y profesionales en el campo de la Geología.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14023483-8dfb-4dee-ba17-8c3d6eaa8d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **¿Qué es un árbol de decisión?**\n",
    "***\n",
    "\n",
    "De acuerdo con [IBM](https://www.ibm.com/es-es/topics/decision-trees#:~:text=Un%20%C3%A1rbol%20de%20decisi%C3%B3n%20es,nodos%20internos%20y%20nodos%20hoja.), un árbol de decisión es un <span style=\"color:gold\">algoritmo de aprendizaje supervisado no paramétrico</span>, que se utiliza tanto para tareas de clasificación como de regresión. Tiene una estructura de árbol jerárquica, que consta de un nodo raíz, ramas, nodos internos y nodos hoja.\n",
    "\n",
    "<img src=\"resources/decision_tree.png\" alt=\"Árbol de decisión\" width=\"700\"/>\n",
    "\n",
    "El aprendizaje del árbol de decisiones emplea una estrategia de **divide y vencerás** mediante la realización de una búsqueda codiciosa para identificar los puntos de división óptimos dentro de un árbol. Este proceso de división se repite de forma recursiva de arriba hacia abajo hasta que todos o la mayoría de los registros se hayan clasificado bajo etiquetas de clase específicas.\n",
    "\n",
    "Que todos los puntos de datos se clasifiquen o no como conjuntos homogéneos depende en gran medida de la complejidad del árbol de decisión.\n",
    "\n",
    "Los árboles más pequeños tienden a generar nodos hoja puros, es decir, puntos de datos en una sola clase.\\\n",
    "Sin embargo, a medida que un árbol crece en tamaño, se vuelve cada vez más difícil mantener esta pureza y, por lo general, da como resultado que haya muy pocos datos dentro de un subárbol determinado.\\\n",
    "Cuando esto ocurre, se conoce como fragmentación de datos y, a menudo, puede resultar en **sobreajustes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42daf84-8039-40d4-b4da-62b11229a2f2",
   "metadata": {},
   "source": [
    "<img src=\"resources/decision_tree_geology.png\" alt=\"Árbol de decisión geológico\" width=\"700\"/>\n",
    "\n",
    "Como resultado, los árboles de decisión tienen preferencia por los árboles pequeños, lo cual es consistente con el principio de parsimonia en la Navaja de Occam. Es decir, \"las entidades no deben multiplicarse más allá de la necesidad\". Dicho de otra manera, los árboles de decisión deben agregar complejidad solo si es necesario, ya que la explicación más simple suele ser la mejor. Para reducir la complejidad y evitar el sobreajuste, generalmente se emplea la **poda (pruning)**.\n",
    "\n",
    "La poda es un proceso que elimina las ramas que se dividen en características con poca importancia. Luego, el ajuste del modelo se puede evaluar mediante el proceso de validación cruzada.\n",
    "\n",
    "### **¿Qué pasaría si utilizamos varios árboles de decisión para predecir un resultado?**\n",
    "\n",
    "Otra forma en que los árboles de decisión pueden mantener su precisión es mediante la formación de un conjunto a través de un algoritmo de **bosque aleatorio (Random Forest)**. Este clasificador predice resultados más precisos, particularmente cuando los árboles individuales no están correlacionados entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0ac0e-c888-4b50-8e3d-299d43c5c9a5",
   "metadata": {},
   "source": [
    "## **Árboles de decisión en Python**\n",
    "\n",
    "Empezaremos importando `pandas` para cargar el archivo `rocas.csv`.\\\n",
    "También importaremos algunas funciones de Sci-Kit Learn:\n",
    "> **Sci-Kit Learn** es una librería utilizada para construir algoritmos de machine learning, la referenciamos dentro de Python como `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce8d91-7aab-459a-91f8-f3c7adc503dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier      # Modelo de Árbol de Decisión\n",
    "from sklearn.model_selection import train_test_split # Función para dividir los datos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score           # Función para medir la precisión del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f98fa-d079-4510-b1b1-dce369db0d4f",
   "metadata": {},
   "source": [
    "Cargamos el archivo `rocas.csv` ubicado en la carpeta `files`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf9c9-6060-43d1-afbb-0ce400a3349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"files/rocas.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70576e7e-0feb-44b1-8c34-79cc5d3b2b68",
   "metadata": {},
   "source": [
    "Ahora, seleccionamos las litologías de `Granodiorita` y `Andesita`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de64518-0d05-49d1-a0a3-269ebf98bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"Nombre\"] == \"Granodiorita\") | (data[\"Nombre\"] == \"Andesita\")].reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f28a7f-e559-456d-8ee2-6ec351278cc8",
   "metadata": {},
   "source": [
    "Crearemos una nueva columna llamada `Roca` y asignaremos un valor numérico que representará el tipo de litología: 0 para `Granodiorita` y 1 para `Andesita`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9471cc1-da90-452f-809f-32093c0cc3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Roca\"] = 0                                                   # Todos los valores en Roca = 0\n",
    "data.loc[data[data[\"Nombre\"] == \"Andesita\"].index, \"Roca\"] = 1     # Valores de Andesita en Roca = 1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf022f1-cab0-4f24-be2c-b4ca2ca0a85f",
   "metadata": {},
   "source": [
    "Luego de hacer la transformación de datos, separamos las columnas de la siguiente forma:\n",
    "- `X` : contiene la información numérica de concentraciones geoquímicas, usada para entrenar y probar el modelo.\n",
    "- `y` : contiene la información de la columna `Roca`, la variable a predecir.\n",
    "\n",
    "Usaremos el atributo `values` del DataFrame para convertir la información en arreglos de Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c3694-4815-4a04-b3f4-e0991f708129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:-1].values\n",
    "y = data[\"Roca\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579e5db-5f2b-443c-b6cf-d09fcb3ff6dc",
   "metadata": {},
   "source": [
    "Una vez separado los datos, procedemos a separar la data de entrenamiento y de prueba usando la función `train_test_split`, el parámetro `test_size=0.10` representa la fracción de la data que será asignada al conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aff931-d40a-4cd1-b5d7-c1f21f835a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb241b-c865-4880-88d6-f735a7f2d381",
   "metadata": {},
   "source": [
    "Ahora, crearemos el modelo y lo entrenaremos usando la data de entrenamiento:\n",
    "> El parámetro `criterion=\"gini\"` establece el criterio de división de los nodos y `max_depth=3` establece la profundidad del árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c350-4040-4545-b78a-533dcb69240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e5208-28b6-459a-a1d4-b4db69db6c6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **¿Qué es la impureza de Gini?**\n",
    "**CART** es una abreviatura de **árboles de clasificación y regresión (classification and regression trees)** y fue introducido por Leo Breiman.\\\n",
    "Este algoritmo generalmente utiliza la **impureza de Gini** para identificar el atributo ideal para la división.\\\n",
    "La impureza de Gini mide la frecuencia con la que se clasifica incorrectamente un atributo elegido al azar. Cuando se evalúa usando la impureza de Gini, un valor más bajo es más ideal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fa4e2-e6c0-4b70-afb1-b761ccfb7217",
   "metadata": {},
   "source": [
    "Procedemos a entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eab57-2a95-4455-8f82-38288eb47a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3925b-2abe-4f82-a41a-3532bfe9c83d",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, mediremos su exactitud usando la función `accuracy_score`:\n",
    "> La **exactitud** representa la cantidad de predicciones que fueron correctas.\\\n",
    "> El parámetro `y_true` representa la data que se busca obtener y `y_pred` es la predicción realizada por el modelo.\\\n",
    "> Para predecir valores con el modelo, tenemos que usar el método `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a4f5a-b871-47f7-a8b0-dd4e2274468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Score - Entrenamiento: {accuracy_score(y_true=y_train, y_pred=model.predict(X_train)):.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {accuracy_score(y_true=y_test, y_pred=model.predict(X_test)):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279c233-54e7-4e0b-9d54-ac667fb55717",
   "metadata": {},
   "source": [
    "### **¿Podemos visualizar un árbol de decisión?**\n",
    "La respuesta es sí, para esto, utilizaremos las funciones `export_text` y `plot_tree` del módulo `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80136f70-0888-499e-9533-0ba0dd01226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_text, plot_tree      # Funciones para graficar el árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b760ecff-39dc-44dc-9caf-c136d50cc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a216f14-843b-485d-8dc9-32c7a1041695",
   "metadata": {},
   "source": [
    "Crearemos una variable llamada `x_cols` para almacenar los nombres de las columnas de X:\n",
    "> Seleccionamos las columnas a partir de la segunda posición (1) hasta antes del último (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcbbd7-4a9a-43ea-ad53-259d25339d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas de la matriz X\n",
    "x_cols = data.iloc[:, 1:-1].columns\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fb186-92a0-45ef-90a0-df9d95266c79",
   "metadata": {},
   "source": [
    "Exportamos los parámetros del árbol de decisión usando la función `export_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1b855-198b-4ff5-8b32-b6250fc17c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = export_text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2374f2-58ee-4b41-b147-177b86f5740e",
   "metadata": {},
   "source": [
    "Ahora, procedemos a graficar el árbol de decisión usando la función `plot_tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee882-64b1-4d69-9056-43e90a87213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "plot_tree(model, feature_names=x_cols, class_names=[\"Granodiorita\", \"Andesita\"], filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4606c13-42bb-4434-a58e-60e55ea30075",
   "metadata": {},
   "source": [
    "Por último, observaremos la importancia de cada columna usando el atributo `feature_importances_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016613f-722c-4e7a-ad15-cf097e4cbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importancia de atributos\")\n",
    "for col, imp in zip(x_cols, model.feature_importances_):\n",
    "    print(f\"{col}: {imp:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fc0fe-4285-4342-9155-a8e9f0ab0c35",
   "metadata": {},
   "source": [
    "La **importancia de atributos** nos ayuda a determinar qué variables son las más importantes para entrenar el modelo.\\\n",
    "Observamos que la columna `SiO2` tiene una importancia muy alta comparada con el resto de columnas.\\\n",
    "Algunas columnas son irrelevantes para el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a14e0d-f144-49f8-bd95-7979897a5634",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Conclusiones**\n",
    "- La naturaleza jerárquica de un árbol de decisión facilita ver qué atributos son los más importantes, por lo que son **más fáciles de interpretar** que otros modelos de aprendizaje automático.\n",
    "- Los árboles de decisión tienen una serie de características que los hacen más flexibles que otros clasificadores.\n",
    "- Los árboles de decisión se pueden aprovechar para tareas de clasificación y regresión, lo que los hace más flexibles que otros algoritmos.\n",
    "- Los árboles de decisión complejos tienden a sobreajustarse y no se generalizan bien a los nuevos datos. Este escenario se puede evitar mediante el proceso de poda (pruning).\n",
    "- Pequeñas variaciones dentro de los datos pueden producir un árbol de decisión muy diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167d194-a28a-4528-8d5b-ffc6406832a4",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
